{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb58472-2aa5-4c68-88e1-7045ca21547f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SaprkTest2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3aaa09e-1abd-41ae-8646-43ea17f8f93d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampath', 'pavan', 'rohith', 'rahul', 'sneha']\n['SAMPATH', 'PAVAN', 'ROHITH', 'RAHUL', 'SNEHA']\n"
     ]
    }
   ],
   "source": [
    "my_list = ['sampath', 'pavan', 'rohith', 'rahul', 'sneha']\n",
    "rdd = spark.sparkContext.parallelize(my_list)\n",
    "print(rdd.collect())\n",
    "\n",
    "new_rdd = rdd.map(lambda x: x.upper())\n",
    "print(new_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61131c5c-3af1-42b6-8d03-290c7ad9db82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "my_list2 = [1,2,3,4,5,6,7,8,9]\n",
    "rdd2 = spark.sparkContext.parallelize(my_list2)\n",
    "print(rdd2.collect())\n",
    "\n",
    "new_rdd2 = rdd2.map(lambda x: x * x)\n",
    "print(new_rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6b778f-79c1-469a-a71c-d03b094f6f0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8])\n",
    "filter_rdd = rdd.filter(lambda x: x%2 == 0)\n",
    "print(filter_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f613ce5d-6276-4f47-889c-8ae7f83575ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello ,', 'world !', 'sampath pavan', 'chaganti']\n['hello', ',', 'world', '!', 'sampath', 'pavan', 'chaganti']\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([\"hello ,\", \"world !\", \"sampath pavan\", \"chaganti\"])\n",
    "print(rdd.collect())\n",
    "fm_rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(fm_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa31b35-5f95-4264-a9d6-9282d988480d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 50000), (30, 70000), (25, 60000), (35, 90000), (30, 80000)]\n[(25, 110000), (35, 90000), (30, 150000)]\n"
     ]
    }
   ],
   "source": [
    "my_list3 = [(25, 50000),(30, 70000),(25, 60000),(35, 90000),(30, 80000)]\n",
    "rdd =spark.sparkContext.parallelize(my_list3)\n",
    "grouped_rdd = rdd.groupByKey()\n",
    "grouped_rdd1 = grouped_rdd.mapValues(lambda x: sum(x))\n",
    "\n",
    "print(my_list3)\n",
    "print(grouped_rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497ad4f4-b39a-4705-a311-878ec328cff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 50000), (30, 70000), (25, 60000), (35, 90000), (30, 80000)]\n[(25, 110000), (35, 90000), (30, 150000)]\n"
     ]
    }
   ],
   "source": [
    "my_list4 = [(25, 50000),(30, 70000),(25, 60000),(35, 90000),(30, 80000)]\n",
    "rdd =spark.sparkContext.parallelize(my_list4)\n",
    "grouped_rdd2 = rdd.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "print(my_list4)\n",
    "print(grouped_rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597c8049-368e-4bd8-a6a0-6bfdfdc681b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, (('Alice', 25), ('New York', 'Engineer'))), (2, (('Bob', 27), ('Australia', 'Artist'))), (3, (('Charlie', 45), ('India', 'Doctor')))]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([(1, ('Alice',25)), (2, ('Bob', 27)), (3, ('Charlie', 45))])\n",
    "rdd3 = spark.sparkContext.parallelize([(1, ('New York','Engineer')), (2, ('Australia', 'Artist')), (3, ('India', 'Doctor'))])\n",
    "jopined_rdd = rdd1.join(rdd3)\n",
    "\n",
    "print(jopined_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456f2628-597f-448c-ab46-9a4ce5f364ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 42, 3, 12, 54]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,1,1,12,2,2,2,3,3,3,54,3,42])\n",
    "distinct_rdd = rdd.distinct()\n",
    "print(distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a004f703-dac8-4b7a-8a38-773fd9d79281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2342334, 687453, 23423, 54, 42, 23, 12, 3, 3, 2, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,23,1,12,2,23423,2,2342334,3,687453,54,3,42])\n",
    "sorted_rdd = rdd.sortBy(lambda x: x, ascending = False)\n",
    "print(sorted_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e006f228-cd1d-44af-8152-7731c6b2f855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([1,2,3])\n",
    "rdd2 = spark.sparkContext.parallelize([4,5,6])\n",
    "\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "print(union_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7925f1f-e7f7-4fa8-bae2-c25de7e2f9a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([1,2,3])\n",
    "rdd2 = spark.sparkContext.parallelize(['a','b','c'])\n",
    "\n",
    "catesian_rdd = rdd1.cartesian(rdd2)\n",
    "print(catesian_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48753c35-50a8-4b00-a001-e73cb9472e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n10\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "new_rdd3 = rdd.repartition(10)\n",
    "print(new_rdd3.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4585eff-01da-49f9-80eb-5da06f87e6ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n8\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "new_rdd4 = rdd.coalesce(10)\n",
    "print(new_rdd4.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c68b6b-0140-4f07-b4ec-06420befc1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we are happy because all are happy, this is beacuse of learning.', 'tommorow i will learn more of sql database.']\n['we', 'are', 'happy', 'because', 'all', 'are', 'happy,', 'this', 'is', 'beacuse', 'of', 'learning.', 'tommorow', 'i', 'will', 'learn', 'more', 'of', 'sql', 'database.']\n[('we', 1), ('are', 1), ('happy', 1), ('because', 1), ('all', 1), ('are', 1), ('happy,', 1), ('this', 1), ('is', 1), ('beacuse', 1), ('of', 1), ('learning.', 1), ('tommorow', 1), ('i', 1), ('will', 1), ('learn', 1), ('more', 1), ('of', 1), ('sql', 1), ('database.', 1)]\n[('we', 1), ('are', 2), ('happy,', 1), ('this', 1), ('is', 1), ('beacuse', 1), ('of', 2), ('learning.', 1), ('tommorow', 1), ('i', 1), ('more', 1), ('database.', 1), ('happy', 1), ('because', 1), ('all', 1), ('will', 1), ('learn', 1), ('sql', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd_words = spark.sparkContext.textFile(\"/FileStore/tables/sparktext.txt\")\n",
    "print(rdd_words.collect())\n",
    "\n",
    "wordcount_rdd = rdd_words.flatMap(lambda x: x.split(\" \"))\n",
    "print(wordcount_rdd.collect())\n",
    "\n",
    "wordcount_rdd2 = wordcount_rdd.map(lambda x: (x,1))\n",
    "print(wordcount_rdd2.collect())\n",
    "\n",
    "wordcount_rdd3 = wordcount_rdd2.reduceByKey(lambda x,y: x+y)\n",
    "print(wordcount_rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed05de7-c1ce-47ae-b83c-00969aeb087a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 1), ('are', 2), ('happy,', 1), ('this', 1), ('is', 1), ('beacuse', 1), ('of', 2), ('learning.', 1), ('tommorow', 1), ('i', 1), ('more', 1), ('database.', 1), ('happy', 1), ('because', 1), ('all', 1), ('will', 1), ('learn', 1), ('sql', 1)]\n"
     ]
    }
   ],
   "source": [
    "# or in single line code\n",
    "\n",
    "single_wordcount_rdd = rdd_words.flatMap(lambda x: x.split(\" \")).map(lambda x: (x,1)).reduceByKey(lambda x,y:x+y).collect()\n",
    "print(single_wordcount_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0f858b-83c3-421e-8c1d-c20bc97b6c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rdd_transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
